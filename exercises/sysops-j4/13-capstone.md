# üéØ Exercice 13 : Plateforme IA S√©curis√©e (Capstone)

> üî¥ Niveau : Avanc√© | ‚è±Ô∏è Dur√©e : 60 min

## Objectif

D√©ployer une plateforme IA s√©curis√©e utilisant tout ce que vous avez appris :
- **Anonymisation** des donn√©es sensibles avant envoi
- **Proxy LLM** pour acc√®s unifi√© √† Claude/GPT/Gemini
- **Infrastructure** Terraform + configuration Ansible

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Plateforme IA S√©curis√©e                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   Utilisateur ‚îÄ‚îÄ‚ñ∫ Anonymizer ‚îÄ‚îÄ‚ñ∫ LiteLLM ‚îÄ‚îÄ‚ñ∫ Claude/GPT/Gemini   ‚îÇ
‚îÇ                   (Scrubadub)     (Proxy)     (APIs publiques)   ‚îÇ
‚îÇ                   :5001           :8000                          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Pr√©requis

- Tous les exercices pr√©c√©dents compl√©t√©s
- VM cloud disponible (exercice 05)
- Docker, Terraform, Ansible fonctionnels
- **API Key** fournie par le formateur (OpenAI, Anthropic ou Google)

## Instructions

### √âtape 1 : Explorer la stack IA (10 min)

1. **D√©couvrir le dossier capstone**
   ```bash
   cd ~/chemin/vers/denvr/capstone
   ls -la
   ```

2. **Comprendre les composants**

   | Fichier | R√¥le |
   |---------|------|
   | `docker-compose.yml` | Orchestre les 2 services |
   | `anonymizer/` | Service Python de masquage PII |
   | `litellm-config.yaml` | Configuration des mod√®les LLM |
   | `.env.example` | Template pour les API keys |

3. **Analyser l'anonymizer**
   ```bash
   cat anonymizer/app.py
   cat anonymizer/Dockerfile
   ```

   **Questions :**
   - [ ] Quel framework Python est utilis√© ?
   - [ ] Quels types de PII sont d√©tect√©s ?
   - [ ] Le Dockerfile utilise-t-il un multi-stage build ?

### √âtape 2 : Tester en local (15 min)

1. **Configurer les API keys**
   ```bash
   cp .env.example .env
   # √âditer .env avec la cl√© fournie par le formateur
   nano .env
   ```

2. **Lancer la stack**
   ```bash
   docker-compose up -d --build
   docker-compose ps
   ```

3. **Tester l'anonymizer**
   ```bash
   # Health check
   curl http://localhost:5001/health
   
   # Anonymiser du texte
   curl -X POST http://localhost:5001/anonymize \
     -H "Content-Type: application/json" \
     -d '{"text": "Mon email est jean.dupont@entreprise.fr et mon tel 0612345678"}'
   ```

   **R√©sultat attendu :**
   ```json
   {
     "anonymized": "Mon email est {{EMAIL}} et mon tel {{PHONE}}",
     "original_length": 58,
     "anonymized_length": 42
   }
   ```

4. **Tester LiteLLM**
   ```bash
   # Liste des mod√®les disponibles
   curl http://localhost:8000/v1/models
   
   # Chat completion
   curl -X POST http://localhost:8000/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
       "model": "gpt-3.5-turbo",
       "messages": [{"role": "user", "content": "Bonjour, qui es-tu?"}]
     }'
   ```

### √âtape 3 : Flow complet s√©curis√© (15 min)

1. **Cr√©er un script de test**
   ```bash
   cat > test-flow.sh << 'EOF'
   #!/bin/bash
   
   # Texte avec des donn√©es sensibles
   TEXT="Bonjour, je suis Jean Dupont. Mon email est jean.dupont@entreprise.fr"
   
   echo "üìù Texte original:"
   echo "$TEXT"
   echo ""
   
   # √âtape 1: Anonymiser
   echo "üîí Anonymisation..."
   ANON=$(curl -s -X POST http://localhost:5001/anonymize \
     -H "Content-Type: application/json" \
     -d "{\"text\": \"$TEXT\"}" | jq -r '.anonymized')
   
   echo "Texte anonymis√©: $ANON"
   echo ""
   
   # √âtape 2: Envoyer au LLM
   echo "ü§ñ Envoi au LLM..."
   RESPONSE=$(curl -s -X POST http://localhost:8000/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d "{
       \"model\": \"gpt-3.5-turbo\",
       \"messages\": [{\"role\": \"user\", \"content\": \"$ANON\"}]
     }" | jq -r '.choices[0].message.content')
   
   echo "R√©ponse LLM: $RESPONSE"
   EOF
   
   chmod +x test-flow.sh
   ./test-flow.sh
   ```

2. **V√©rifier la protection des donn√©es**
   - Le texte envoy√© au LLM ne contient-il plus de PII ?
   - Les donn√©es sensibles restent-elles sur votre infrastructure ?

### √âtape 4 : D√©ployer sur le cloud (20 min)

1. **Pr√©parer le d√©ploiement Ansible**
   ```bash
   # Cr√©er un playbook pour d√©ployer la stack IA
   cat > deploy-ai-platform.yml << 'EOF'
   ---
   - name: Deploy AI Platform
     hosts: webservers
     become: true
     vars:
       app_dir: /opt/ai-platform
     
     tasks:
       - name: Create app directory
         ansible.builtin.file:
           path: "{{ app_dir }}"
           state: directory
           mode: '0755'
       
       - name: Copy docker-compose files
         ansible.builtin.copy:
           src: "{{ item }}"
           dest: "{{ app_dir }}/"
         loop:
           - docker-compose.yml
           - litellm-config.yaml
           - .env
       
       - name: Copy anonymizer folder
         ansible.builtin.copy:
           src: anonymizer/
           dest: "{{ app_dir }}/anonymizer/"
       
       - name: Start services
         community.docker.docker_compose:
           project_src: "{{ app_dir }}"
           state: present
           build: true
   EOF
   ```

2. **D√©ployer**
   ```bash
   ansible-playbook -i inventory deploy-ai-platform.yml
   ```

3. **Tester depuis le cloud**
   ```bash
   curl http://VM_IP:5001/health
   curl http://VM_IP:8000/v1/models
   ```

---

## üß™ Validation

‚úÖ Vous avez r√©ussi si :
- [ ] L'anonymizer masque correctement les emails et t√©l√©phones
- [ ] LiteLLM r√©pond aux requ√™tes chat
- [ ] Le flow complet (anonymize ‚Üí LLM) fonctionne
- [ ] La stack est d√©ploy√©e sur le cloud

---

## üí° Troubleshooting

| Probl√®me | Solution |
|----------|----------|
| `anonymizer` ne d√©marre pas | V√©rifier `docker-compose logs anonymizer` |
| LiteLLM erreur 401 | V√©rifier les API keys dans `.env` |
| Requ√™te timeout | API key invalide ou quota d√©pass√© |
| Port d√©j√† utilis√© | `docker-compose down` puis relancer |

---

## ‚úÖ Solution

<details>
<summary>Commandes compl√®tes</summary>

```bash
cd capstone

# Configuration
cp .env.example .env
# √âditer .env

# Lancement
docker-compose up -d --build

# Tests
curl http://localhost:5001/health
curl -X POST http://localhost:5001/anonymize \
  -H "Content-Type: application/json" \
  -d '{"text": "Email: test@example.com"}'

curl http://localhost:8000/v1/models
```

</details>

---

## ü§ñ R√©flexion finale

√Ä la fin de cet exercice, r√©fl√©chissez :

> *"Pourquoi anonymiser les donn√©es avant de les envoyer √† un LLM externe ?"*

**Raisons :**
1. **RGPD** : Les donn√©es personnelles ne doivent pas quitter l'UE sans garanties
2. **Confidentialit√©** : Les LLMs peuvent m√©moriser les donn√©es d'entra√Ænement
3. **S√©curit√©** : R√©duire la surface d'attaque en cas de breach chez le provider
4. **Conformit√©** : Exigences internes de l'entreprise

**Ce que vous avez appris :**
- ‚úÖ Construire un service de protection des donn√©es
- ‚úÖ Utiliser un proxy pour unifier l'acc√®s aux LLMs
- ‚úÖ D√©ployer une stack compl√®te avec Docker Compose
- ‚úÖ S√©curiser une infrastructure DevSecOps

---

## üéì F√©licitations !

Vous avez compl√©t√© le workshop DevSecOps et d√©ploy√© votre propre plateforme IA s√©curis√©e !

**Prochaines √©tapes sugg√©r√©es :**
- [ ] Ajouter une UI web (Open WebUI)
- [ ] Impl√©menter le logging des requ√™tes
- [ ] Ajouter de l'authentification (API keys)
- [ ] Explorer d'autres d√©tecteurs Scrubadub (noms, adresses)
